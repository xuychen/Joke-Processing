{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected: 10\n",
      "train R2 : 0.745\n",
      "test R2 : 0.588\n"
     ]
    }
   ],
   "source": [
    "# original\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun 25 21:16:14 2018\n",
    "\n",
    "@author: T\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor \n",
    "\n",
    "#X = data.drop(['joke1'], axis = 1)\n",
    "#y = data['joke1']\n",
    "#first few variables of the first few principal components\n",
    "#princomp = pd.DataFrame(pca.components_.T).iloc[:65,:1]\n",
    "#top 10 features of first principal component\n",
    "#pcafeatures = []\n",
    "#for item in princomp.sort_values(by=[0], ascending=False).index.tolist()[:10]:\n",
    "#    pcafeatures.append(X.columns[item])\n",
    "\n",
    "def PCR(regr, labeling):\n",
    "    pca = PCA()\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=100)\n",
    "    \n",
    "    X_reduced_train = pca.fit_transform(train_X)\n",
    "    print X_reduced_train\n",
    "    n = len(X_reduced_train)\n",
    "    kf_10 = model_selection.KFold(n, n_folds=10, shuffle=True, random_state=1)\n",
    "    r2 = []\n",
    "    score = -1*cross_val_score(regr, np.ones((n,1)), train_y.ravel(), cv=kf_10, scoring='r2').mean()\n",
    "    r2.append(score)\n",
    "    \n",
    "    for i in np.arange(1, 20):\n",
    "        score = -1*model_selection.cross_val_score(regr, X_reduced_train[:,:i], train_y.ravel(), cv=kf_10, scoring='r2').mean()\n",
    "        r2.append(score)\n",
    "    \n",
    "    plt.plot(r2, '-v', label=labeling)\n",
    "    plt.xlabel('Number of principal components in regression')\n",
    "    plt.ylabel('R2')\n",
    "    plt.title('Predicting Joke 1')\n",
    "    plt.xlim(xmin=-1);\n",
    "    plt.legend()\n",
    "    \n",
    "    np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "    numcomp = r2.index(max(r2))\n",
    "    print(\"Number of principal components: {}\" .format(numcomp))\n",
    "    \n",
    "    X_reduced_test = pca.transform(test_X)[:,:numcomp+1]\n",
    "    X_reduced_train = pca.transform(train_X)[:,:numcomp+1]\n",
    "    regr.fit(X_reduced_train[:,:numcomp+1], train_y)\n",
    "    pred_train = regr.predict(X_reduced_train)\n",
    "    pred_test = regr.predict(X_reduced_test)\n",
    "    print(\"{} train R2 : {:.3f}\" .format(labeling, r2_score(train_y, pred_train)))\n",
    "    print(\"{} test R2 : {:.3f}\" .format(labeling, r2_score(test_y, pred_test)))\n",
    "#    print(\"MSE: {:.3f}\".format(mean_squared_error(test_y, pred)))\n",
    "#    print('train R2 : {:.3f}'.format(regr.score(train_y, pred_train)))\n",
    "\n",
    "def r2(pred, actual):\n",
    "    RSS = 0\n",
    "    TSS = 0\n",
    "    for i in range(0, len(pred)):\n",
    "        TSS = TSS + ((actual[i] - np.mean(actual))**2)\n",
    "        RSS = RSS + ((actual[i] - pred[i])**2)\n",
    "    R2 = 1-RSS/TSS\n",
    "    return R2\n",
    "\n",
    "def mse(y, y_pred):\n",
    "    return np.mean((y-y_pred)**2)\n",
    "\n",
    "def evaluation(regr, labeling, ranked_features):\n",
    "    totaltrainr2 = []\n",
    "    totaltestr2 = []\n",
    "    totaltrainmse = []\n",
    "    totaltestmse = []\n",
    "    min_test_mse = 0\n",
    "    max_test_r2 = 0\n",
    "    num_features_to_use = 0\n",
    "    \n",
    "    #iterating over the ranked features from most to least important\n",
    "    for i in range(1,len(ranked_features)):\n",
    "        trainr2 = []\n",
    "        testr2 = []\n",
    "        mse_train = []\n",
    "        mse_test = []\n",
    "        test_features = ranked_features[:i]\n",
    "        n = len(X[test_features])\n",
    "\n",
    "        from sklearn.model_selection import KFold \n",
    "        kf = KFold(n_splits=7, shuffle=True) \n",
    "        \n",
    "        for train_index, test_index in kf.split(X[test_features]):\n",
    "              train_X, test_X = X[test_features].iloc[train_index], X[test_features].iloc[test_index] \n",
    "              train_y, test_y = y.iloc[train_index], y.iloc[test_index]     \n",
    "             \n",
    "              regr.fit(train_X, train_y)\n",
    "              pred_train = regr.predict(train_X)\n",
    "              pred_test = regr.predict(test_X)\n",
    "              \n",
    "              trainr2.append(r2(pred_train,list(train_y)))\n",
    "              testr2.append(r2(pred_test, list(test_y)))              \n",
    "              \n",
    "              mse_train.append(mse(train_y, list(pred_train)))              \n",
    "              mse_test.append(mse(test_y, list(pred_test)))   \n",
    "          \n",
    "        totaltrainr2.append(np.mean(trainr2))\n",
    "        totaltestr2.append(np.mean(testr2))\n",
    "        totaltrainmse.append(np.mean(mse_train))\n",
    "        totaltestmse.append(np.mean(mse_test))\n",
    "        \n",
    "#        if min_test_mse == 0:\n",
    "#            min_test_mse = np.mean(mse_test)\n",
    "        if max_test_r2 == 0:\n",
    "            max_test_r2 = np.mean(testr2)\n",
    "#        if np.mean(mse_test) < min_test_mse:\n",
    "        if np.mean(testr2) > max_test_r2:\n",
    "#            min_test_mse = np.mean(mse_test)\n",
    "#            min_train_mse = np.mean(mse_train)\n",
    "            max_test_r2 = np.mean(testr2)\n",
    "            max_train_r2 = np.mean(trainr2)\n",
    "            num_of_features_to_use = i\n",
    "            used_train_y = train_y\n",
    "            used_pred_train = pred_train\n",
    "            used_test_y = test_y\n",
    "            used_pred_test = pred_test\n",
    "            \n",
    "    print(\"Number of features selected: {}\".format(num_of_features_to_use))\n",
    "    print(\"train R2 : {:.3f}\" .format(max_train_r2))\n",
    "    print(\"test R2 : {:.3f}\" .format(max_test_r2))    \n",
    "#    print(\"train MSE: {:.3f}\".format(min_train_mse))\n",
    "#    print(\"test MSE: {:.3f}\".format(min_test_mse))\n",
    "    \n",
    "    #pred v. actual & residual plots        \n",
    "    plt.scatter(used_train_y, used_pred_train, label='training set')\n",
    "    plt.scatter(used_test_y, used_pred_test, label='test set')\n",
    "    plt.plot( [-1,1],[-1,1] )\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('Prediction plot for Joke 1')        \n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(5,5))    \n",
    "\n",
    "    plt.scatter(used_pred_train, used_pred_train-used_train_y, label='training set')\n",
    "    plt.scatter(used_pred_test, used_pred_test-used_test_y, label='test set')\n",
    "    plt.hlines(y=0, xmin=-1, xmax=1)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals plot for Joke 1')\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(5,5))    \n",
    "\n",
    "    #Training plots\n",
    "#    plt.plot(totaltrainmse, '-v', label=labeling)\n",
    "#    plt.xlabel('Number of features')\n",
    "#    plt.ylabel('MSE training')\n",
    "#    plt.title('Predicting Joke 1')\n",
    "#    plt.xlim(xmin=-1);\n",
    "#    plt.legend()\n",
    "#    plt.figure(figsize=(5,5))\n",
    "\n",
    "    plt.plot(totaltrainr2, '-v', label=labeling)\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('R2 training')\n",
    "    plt.title('Predicting Joke 1')\n",
    "    plt.xlim(xmin=-1);\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    #Testing plots\n",
    "#    plt.plot(totaltestmse, '-v', label=labeling)\n",
    "#    plt.xlabel('Number of features')\n",
    "#    plt.ylabel('MSE testing')\n",
    "#    plt.title('Predicting Joke 1')\n",
    "#    plt.xlim(xmin=-1);\n",
    "#    plt.legend()\n",
    "#    plt.figure(figsize=(5,5))    \n",
    "    \n",
    "    plt.plot(totaltestr2, '-v', label=labeling)\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('R2 testing')\n",
    "    plt.title('Predicting Joke 1')\n",
    "    plt.xlim(xmin=-1);\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(5,5))    \n",
    "    \n",
    "    \n",
    "def rank_to_dict(ranks, names, order=1):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(names, ranks ))\n",
    "\n",
    "def list_features(key, features, names):\n",
    "    featurelist = []\n",
    "    for i in features.sort_values(key, ascending = False).index.values:\n",
    "        featurelist.append(names[i])\n",
    "    return(featurelist)         \n",
    "\n",
    "def feature_selection():\n",
    "    from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "    from sklearn.feature_selection import RFE, f_regression\n",
    "     \n",
    "    np.random.seed(0)\n",
    "    names = X.columns\n",
    "    ranks = {}\n",
    "\n",
    "    lr = LinearRegression(fit_intercept=True,normalize=False)         \n",
    "    lr.fit(X, y)\n",
    "    ranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), names)\n",
    "     \n",
    "    ridge = Ridge(alpha=7)\n",
    "    ridge.fit(X, y)\n",
    "    ranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), names)\n",
    "    print ridge.coef_\n",
    "     \n",
    "    lasso = Lasso(alpha=.05)\n",
    "    lasso.fit(X, y)\n",
    "    ranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), names)\n",
    "      \n",
    "    ##stop the search when 5 features are left (they will get equal scores)\n",
    "    regr = RandomForestRegressor(n_estimators=50, max_features=\"sqrt\", min_samples_leaf=4, max_depth=3, bootstrap=True)\n",
    "    rfe = RFE(regr, n_features_to_select=5)\n",
    "    rfe.fit(X,y)\n",
    "    ranks[\"RFE\"] = rank_to_dict(list(map(float,rfe.ranking_)), names, order=-1)\n",
    "     \n",
    "    rf = RandomForestRegressor(n_estimators=50, max_features=\"sqrt\", min_samples_leaf=4, max_depth=3, bootstrap=True)\n",
    "    rf.fit(X,y)\n",
    "    ranks[\"RF\"] = rank_to_dict(rf.feature_importances_, names)\n",
    "     \n",
    "    f, pval  = f_regression(X, y, center=True)\n",
    "    ranks[\"Corr.\"] = rank_to_dict(f, names)\n",
    "     \n",
    "    r = {}\n",
    "    for name in names:\n",
    "        r[name] = round(np.mean([ranks[method][name] \n",
    "                                 for method in ranks.keys()]), 2)\n",
    "    methods = sorted(ranks.keys())\n",
    "    ranks[\"Mean\"] = r\n",
    "    methods.append(\"Mean\")\n",
    "    \n",
    "    #extract features in order of most to least important\n",
    "    matrix = []\n",
    "    matrix.append(methods)\n",
    "    for name in names:\n",
    "        matrix.append(list(map(str,[ranks[method][name] for method in methods])))\n",
    "    npmat = np.array(matrix)\n",
    "    ranked_features = pd.DataFrame(data=npmat[1:,0:], columns=npmat[0,0:])\n",
    "\n",
    "    #just testing random forest for now\n",
    "    models = [RandomForestRegressor(n_estimators=50, max_features=\"sqrt\", min_samples_leaf=4, max_depth=3, bootstrap=True)] #SVR(kernel=\"sigmoid\", C=0.5, coef0=-0.5)] #MLPRegressor(hidden_layer_sizes = (3),max_iter=500)]    \n",
    "    for model in models: \n",
    "        print(model)\n",
    "        for feature_selection_method in methods:\n",
    "            print(feature_selection_method)\n",
    "#        feature_selection_method = \"RFE\" #Just testing RFE for now    \n",
    "            evaluation(model, feature_selection_method, list_features(feature_selection_method,ranked_features,names))\n",
    "\n",
    "    \n",
    "def LOF(): #Removing outliers\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    \n",
    "    clf = LocalOutlierFactor(n_neighbors=20)\n",
    "    y_pred = clf.fit_predict(data)\n",
    "    outliers = [i for i, x in enumerate(y_pred.tolist()) if x == -1]\n",
    "    print(outliers)\n",
    "    print(data)\n",
    "    data.drop(outliers, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.to_csv(\"dataoutlier.csv\")\n",
    "    \n",
    "\n",
    "data = pd.read_csv('data-LOFapplied.csv')\n",
    "#LOF()\n",
    "jokes = list(data.columns)[list(data.columns).index('joke1'):len(data.columns)]\n",
    "X = data.drop(jokes[75], axis=1)\n",
    "y = data[jokes[75]]\n",
    "feature_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jun 25 21:16:14 2018\n",
    "\n",
    "@author: T\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor \n",
    "\n",
    "#X = data.drop(['joke1'], axis = 1)\n",
    "#y = data['joke1']\n",
    "#first few variables of the first few principal components\n",
    "#princomp = pd.DataFrame(pca.components_.T).iloc[:65,:1]\n",
    "#top 10 features of first principal component\n",
    "#pcafeatures = []\n",
    "#for item in princomp.sort_values(by=[0], ascending=False).index.tolist()[:10]:\n",
    "#    pcafeatures.append(X.columns[item])\n",
    "\n",
    "def PCR(regr, labeling):\n",
    "    pca = PCA()\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, train_size=100)\n",
    "    \n",
    "    X_reduced_train = pca.fit_transform(train_X)\n",
    "    n = len(X_reduced_train)\n",
    "    kf_10 = model_selection.KFold(n, n_folds=10, shuffle=True, random_state=1)\n",
    "    r2 = []\n",
    "    score = -1*cross_val_score(regr, np.ones((n,1)), train_y.ravel(), cv=kf_10, scoring='r2').mean()\n",
    "    r2.append(score)\n",
    "    \n",
    "    for i in np.arange(1, 20):\n",
    "        score = -1*model_selection.cross_val_score(regr, X_reduced_train[:,:i], train_y.ravel(), cv=kf_10, scoring='r2').mean()\n",
    "        r2.append(score)\n",
    "    \n",
    "    plt.plot(r2, '-v', label=labeling)\n",
    "    plt.xlabel('Number of principal components in regression')\n",
    "    plt.ylabel('R2')\n",
    "    plt.title('Predicting Joke 1')\n",
    "    plt.xlim(xmin=-1);\n",
    "    plt.legend()\n",
    "    \n",
    "    np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "    numcomp = r2.index(max(r2))\n",
    "    print(\"Number of principal components: {}\" .format(numcomp))\n",
    "    \n",
    "    X_reduced_test = pca.transform(test_X)[:,:numcomp+1]\n",
    "    X_reduced_train = pca.transform(train_X)[:,:numcomp+1]\n",
    "    regr.fit(X_reduced_train[:,:numcomp+1], train_y)\n",
    "    pred_train = regr.predict(X_reduced_train)\n",
    "    pred_test = regr.predict(X_reduced_test)\n",
    "    print(\"{} train R2 : {:.3f}\" .format(labeling, r2_score(train_y, pred_train)))\n",
    "    print(\"{} test R2 : {:.3f}\" .format(labeling, r2_score(test_y, pred_test)))\n",
    "#    print(\"MSE: {:.3f}\".format(mean_squared_error(test_y, pred)))\n",
    "#    print('train R2 : {:.3f}'.format(regr.score(train_y, pred_train)))\n",
    "\n",
    "def r2(pred, actual):\n",
    "    RSS = 0\n",
    "    TSS = 0\n",
    "    for i in range(0, len(pred)):\n",
    "        TSS = TSS + ((actual[i] - np.mean(actual))**2)\n",
    "        RSS = RSS + ((actual[i] - pred[i])**2)\n",
    "    R2 = 1-RSS/TSS\n",
    "    return R2\n",
    "\n",
    "def mse(y, y_pred):\n",
    "    return np.mean((y-y_pred)**2)\n",
    "\n",
    "def evaluation(regr, labeling, ranked_features):\n",
    "    totaltrainr2 = []\n",
    "    totaltestr2 = []\n",
    "    totaltrainmse = []\n",
    "    totaltestmse = []\n",
    "    min_test_mse = 0\n",
    "    max_test_r2 = 0\n",
    "    num_features_to_use = 0\n",
    "    \n",
    "    #iterating over the ranked features from most to least important\n",
    "    for i in range(1,len(ranked_features)):\n",
    "        trainr2 = []\n",
    "        testr2 = []\n",
    "        mse_train = []\n",
    "        mse_test = []\n",
    "        test_features = ranked_features[:i]\n",
    "        n = len(X[test_features])\n",
    "\n",
    "        from sklearn.model_selection import KFold \n",
    "        kf = KFold(n_splits=7, shuffle=True) \n",
    "        \n",
    "        for train_index, test_index in kf.split(X[test_features]):\n",
    "              train_X, test_X = X[test_features].iloc[train_index], X[test_features].iloc[test_index] \n",
    "              train_y, test_y = y.iloc[train_index], y.iloc[test_index]     \n",
    "             \n",
    "              regr.fit(train_X, train_y)\n",
    "              pred_train = regr.predict(train_X)\n",
    "              pred_test = regr.predict(test_X)\n",
    "              \n",
    "              trainr2.append(r2(pred_train,list(train_y)))\n",
    "              testr2.append(r2(pred_test, list(test_y)))              \n",
    "              \n",
    "              mse_train.append(mse(train_y, list(pred_train)))              \n",
    "              mse_test.append(mse(test_y, list(pred_test)))   \n",
    "          \n",
    "        totaltrainr2.append(np.mean(trainr2))\n",
    "        totaltestr2.append(np.mean(testr2))\n",
    "        totaltrainmse.append(np.mean(mse_train))\n",
    "        totaltestmse.append(np.mean(mse_test))\n",
    "        \n",
    "#        if min_test_mse == 0:\n",
    "#            min_test_mse = np.mean(mse_test)\n",
    "        if max_test_r2 == 0:\n",
    "            max_test_r2 = np.mean(testr2)\n",
    "#        if np.mean(mse_test) < min_test_mse:\n",
    "        if np.mean(testr2) > max_test_r2:\n",
    "#            min_test_mse = np.mean(mse_test)\n",
    "#            min_train_mse = np.mean(mse_train)\n",
    "            max_test_r2 = np.mean(testr2)\n",
    "            max_train_r2 = np.mean(trainr2)\n",
    "            num_of_features_to_use = i\n",
    "            used_train_y = train_y\n",
    "            used_pred_train = pred_train\n",
    "            used_test_y = test_y\n",
    "            used_pred_test = pred_test\n",
    "            \n",
    "    print(\"Number of features selected: {}\".format(num_of_features_to_use))\n",
    "    print(\"train R2 : {:.3f}\" .format(max_train_r2))\n",
    "    print(\"test R2 : {:.3f}\" .format(max_test_r2))    \n",
    "#    print(\"train MSE: {:.3f}\".format(min_train_mse))\n",
    "#    print(\"test MSE: {:.3f}\".format(min_test_mse))\n",
    "    \n",
    "    #pred v. actual & residual plots        \n",
    "    plt.scatter(used_train_y, used_pred_train, label='training set')\n",
    "    plt.scatter(used_test_y, used_pred_test, label='test set')\n",
    "    plt.plot( [-1,1],[-1,1] )\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('Prediction plot for Joke 1')        \n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(5,5))    \n",
    "\n",
    "    plt.scatter(used_pred_train, used_pred_train-used_train_y, label='training set')\n",
    "    plt.scatter(used_pred_test, used_pred_test-used_test_y, label='test set')\n",
    "    plt.hlines(y=0, xmin=-1, xmax=1)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title('Residuals plot for Joke 1')\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(5,5))    \n",
    "\n",
    "    #Training plots\n",
    "#    plt.plot(totaltrainmse, '-v', label=labeling)\n",
    "#    plt.xlabel('Number of features')\n",
    "#    plt.ylabel('MSE training')\n",
    "#    plt.title('Predicting Joke 1')\n",
    "#    plt.xlim(xmin=-1);\n",
    "#    plt.legend()\n",
    "#    plt.figure(figsize=(5,5))\n",
    "\n",
    "    plt.plot(totaltrainr2, '-v', label=labeling)\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('R2 training')\n",
    "    plt.title('Predicting Joke 1')\n",
    "    plt.xlim(xmin=-1);\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(5,5))\n",
    "\n",
    "    #Testing plots\n",
    "#    plt.plot(totaltestmse, '-v', label=labeling)\n",
    "#    plt.xlabel('Number of features')\n",
    "#    plt.ylabel('MSE testing')\n",
    "#    plt.title('Predicting Joke 1')\n",
    "#    plt.xlim(xmin=-1);\n",
    "#    plt.legend()\n",
    "#    plt.figure(figsize=(5,5))    \n",
    "    \n",
    "    plt.plot(totaltestr2, '-v', label=labeling)\n",
    "    plt.xlabel('Number of features')\n",
    "    plt.ylabel('R2 testing')\n",
    "    plt.title('Predicting Joke 1')\n",
    "    plt.xlim(xmin=-1);\n",
    "    plt.legend()\n",
    "    plt.figure(figsize=(5,5))    \n",
    "    \n",
    "    \n",
    "def rank_to_dict(ranks, names, order=1):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    minmax = MinMaxScaler()\n",
    "    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n",
    "    ranks = map(lambda x: round(x, 2), ranks)\n",
    "    return dict(zip(names, ranks ))\n",
    "\n",
    "def list_features(key, features, names):\n",
    "    featurelist = []\n",
    "    for i in features.sort_values(key, ascending = False).index.values:\n",
    "        featurelist.append(names[i])\n",
    "    return(featurelist)         \n",
    "\n",
    "def feature_selection():\n",
    "    from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\n",
    "    from sklearn.feature_selection import RFE, f_regression\n",
    "     \n",
    "    np.random.seed(0)\n",
    "    names = X.columns\n",
    "    ranks = {}\n",
    "\n",
    "    lr = LinearRegression(fit_intercept=True,normalize=False)         \n",
    "    lr.fit(X, y)\n",
    "    ranks[\"Linear reg\"] = rank_to_dict(np.abs(lr.coef_), names)\n",
    "\n",
    "     \n",
    "    ridge = Ridge(alpha=7)\n",
    "    ridge.fit(X, y)\n",
    "    ranks[\"Ridge\"] = rank_to_dict(np.abs(ridge.coef_), names)\n",
    "    lasso = Lasso(alpha=.05)\n",
    "    lasso.fit(X, y)\n",
    "    ranks[\"Lasso\"] = rank_to_dict(np.abs(lasso.coef_), names)\n",
    "    ##stop the search when 5 features are left (they will get equal scores)\n",
    "    regr = RandomForestRegressor(n_estimators=50, max_features=\"sqrt\", min_samples_leaf=4, max_depth=3, bootstrap=True)\n",
    "    print regr\n",
    "    rfe = RFE(regr, n_features_to_select=5)\n",
    "    rfe.fit(X,y)\n",
    "    print rfe.ranking_\n",
    "    ranks[\"RFE\"] = rank_to_dict(list(map(float,rfe.ranking_)), names, order=-1)\n",
    "     \n",
    "    rf = RandomForestRegressor(n_estimators=50, max_features=\"sqrt\", min_samples_leaf=4, max_depth=3, bootstrap=True)\n",
    "    rf.fit(X,y)\n",
    "    ranks[\"RF\"] = rank_to_dict(rf.feature_importances_, names)\n",
    "     \n",
    "    f, pval  = f_regression(X, y, center=True)\n",
    "    ranks[\"Corr.\"] = rank_to_dict(f, names)\n",
    "     \n",
    "    r = {}\n",
    "    for name in names:\n",
    "        r[name] = round(np.mean([ranks[method][name] \n",
    "                                 for method in ranks.keys()]), 2)\n",
    "    methods = sorted(ranks.keys())\n",
    "    ranks[\"Mean\"] = r\n",
    "    methods.append(\"Mean\")\n",
    "    \n",
    "    #extract features in order of most to least important\n",
    "    matrix = []\n",
    "    matrix.append(methods)\n",
    "    for name in names:\n",
    "        matrix.append(list(map(str,[ranks[method][name] for method in methods])))\n",
    "    npmat = np.array(matrix)\n",
    "    ranked_features = pd.DataFrame(data=npmat[1:,0:], columns=npmat[0,0:])\n",
    "\n",
    "    #just testing random forest for now\n",
    "    models = [RandomForestRegressor(n_estimators=50, max_features=\"sqrt\", min_samples_leaf=4, max_depth=3, bootstrap=True)] #SVR(kernel=\"sigmoid\", C=0.5, coef0=-0.5)] #MLPRegressor(hidden_layer_sizes = (3),max_iter=500)]    \n",
    "    for model in models: \n",
    "        print(model)\n",
    "        for feature_selection_method in methods:\n",
    "            print(feature_selection_method)\n",
    "#        feature_selection_method = \"RFE\" #Just testing RFE for now    \n",
    "            evaluation(model, feature_selection_method, list_features(feature_selection_method,ranked_features,names))\n",
    "\n",
    "    \n",
    "def LOF(): #Removing outliers\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    \n",
    "    clf = LocalOutlierFactor(n_neighbors=20)\n",
    "    y_pred = clf.fit_predict(data)\n",
    "    outliers = [i for i, x in enumerate(y_pred.tolist()) if x == -1]\n",
    "    print(outliers)\n",
    "    print(data)\n",
    "    data.drop(outliers, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.to_csv(\"dataoutlier.csv\")\n",
    "    \n",
    "\n",
    "data = pd.read_csv('data-LOFapplied.csv')\n",
    "#LOF()\n",
    "jokes = list(data.columns)[list(data.columns).index('joke1'):len(data.columns)]\n",
    "X = data.drop(jokes[75], axis=1)\n",
    "y = data[jokes[75]]\n",
    "feature_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data-LOFapplied.csv')\n",
    "#LOF()\n",
    "jokes = list(data.columns)[list(data.columns).index('joke1'):len(data.columns)]\n",
    "X = data.drop(jokes[75], axis=1)\n",
    "y = data[jokes[75]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, max_iter=1000):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.max_iter_ = max_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Gradient Descent\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        rows = X.shape[0]\n",
    "        n_dim = X.shape[1]\n",
    "\n",
    "        tf_X = tf.placeholder(tf.float32, [n_dim])\n",
    "        tf_y = tf.placeholder(tf.float32)\n",
    "\n",
    "        # set random weight\n",
    "        W = tf.Variable([0. for i in range(n_dim)], name=\"weight\")\n",
    "        b = tf.Variable(0., name=\"bias\")\n",
    "        learning_rate = 0.01\n",
    "        epochs = self.max_iter_\n",
    "\n",
    "        # Construct a linear model\n",
    "        mult = tf.tensordot(tf_X, W, 1)\n",
    "        pred = tf.add(mult, b)\n",
    "        cost = ((pred-tf_y)** 2)/(2.0*rows)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        # start training\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                for _x, _y in zip(X, y):\n",
    "                    sess.run(optimizer, feed_dict={tf_X: _x, tf_y: _y})\n",
    "\n",
    "            print (\"Optimization Finished\")\n",
    "            self.coef_ = sess.run(W)\n",
    "            self.intercept = sess.run(b)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        X = np.array(X)\n",
    "        for row in X:\n",
    "            result.append(np.dot(self.coef_, row.T) + self.intercept_)\n",
    "            \n",
    "        return result\n",
    "\n",
    "    \n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "pred = model.predict(X)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print model.coef_\n",
    "print mean_squared_error(pred, y)\n",
    "print r2_score(pred, y)\n",
    "\n",
    "# names = X.columns\n",
    "# ranks = {}\n",
    "# ranks[\"Linear reg\"] = rank_to_dict(np.abs(linear_regression(X, y)), names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class Lasso():\n",
    "    def __init__(self, alpha=1.0, max_iter=1000, fit_intercept=True):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "\n",
    "    def _soft_thresholding_operator(self, x, lambda_):\n",
    "        if x > 0 and lambda_ < abs(x):\n",
    "            return x - lambda_\n",
    "        elif x < 0 and lambda_ < abs(x):\n",
    "            return x + lambda_\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.column_stack((np.ones(len(X)),X))\n",
    "\n",
    "        beta = np.zeros(X.shape[1])\n",
    "        if self.fit_intercept:\n",
    "            beta[0] = np.sum(y - np.dot(X[:, 1:], beta[1:]))/(X.shape[0])\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            start = 1 if self.fit_intercept else 0\n",
    "            for j in range(start, len(beta)):\n",
    "                tmp_beta = deepcopy(beta)\n",
    "                tmp_beta[j] = 0.0\n",
    "                r_j = y - np.dot(X, tmp_beta)\n",
    "                arg1 = np.dot(X[:, j], r_j)\n",
    "                arg2 = self.alpha*X.shape[0]\n",
    "\n",
    "                beta[j] = self._soft_thresholding_operator(arg1, arg2)/(X[:, j]**2).sum()\n",
    "\n",
    "            if self.fit_intercept:\n",
    "                beta[0] = np.sum(y - np.dot(X[:, 1:], beta[1:]))/(X.shape[0])\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            self.intercept_ = beta[0]\n",
    "            self.coef_ = beta[1:]\n",
    "        else:\n",
    "            self.coef_ = beta\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y = np.dot(X, self.coef_)\n",
    "        if self.fit_intercept:\n",
    "            y += self.intercept_*np.ones(len(y))\n",
    "        return y\n",
    "\n",
    "model = Lasso(alpha=0.05, max_iter=1000)\n",
    "model.fit(X, y)\n",
    "print model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ridge:\n",
    "    def __init__(self, alpha=1.0, max_iter=1000):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        coefs = self.fit_withLSQR(X, y)\n",
    "        self.coef_, self.intercept = coefs[:-1], coefs[-1]\n",
    "            \n",
    "    def fit_withLSQR(self, X, y, tol=1e-3):\n",
    "        n_samples, n_features = X.shape\n",
    "        ones = np.asarray([np.ones(n_samples)]).T\n",
    "        X = np.concatenate((X, ones), axis=1)\n",
    "\n",
    "        # According to the lsqr documentation, alpha = damp^2.\n",
    "        sqrt_alpha = np.sqrt(self.alpha)\n",
    "        info = sp.sparse.linalg.lsqr(X, y, damp=sqrt_alpha,\n",
    "                              atol=tol, btol=tol, iter_lim=self.max_iter)\n",
    "        return info[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        X = np.array(X)\n",
    "        for row in X:\n",
    "            result.append(np.dot(self.coef_, row.T) + self.intercept_)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "ridge = Ridge(7., 1000)\n",
    "ridge.fit(X, y)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "pred = ridge.predict(X)\n",
    "print mean_squared_error(pred, y)\n",
    "print r2_score(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "185 185\n",
      "[[ -6.71171256e+00  -3.58413330e+00  -1.59364767e+00 ...,  -2.81428167e+00\n",
      "   -2.06242599e-01  -7.58274322e+00]\n",
      " [ -2.25011662e-01  -2.41382736e+00  -4.66608290e+00 ...,  -2.43386677e+00\n",
      "   -4.95516555e+00   1.05055511e+00]\n",
      " [  2.27105994e+00   4.81323496e-01  -1.26965494e+00 ...,   2.39270954e+00\n",
      "    1.84461864e+00   2.42596960e+00]\n",
      " ..., \n",
      " [  5.75928194e-15  -3.93001603e-15   1.38777878e-15 ...,   6.10622664e-16\n",
      "   -3.88578059e-16  -4.71844785e-15]\n",
      " [ -4.92661467e-15   6.16173779e-15  -4.94049246e-15 ...,  -4.55191440e-15\n",
      "   -4.16333634e-15   5.82867088e-15]\n",
      " [  2.60902411e-15  -7.21644966e-15   4.99600361e-16 ...,   2.22044605e-16\n",
      "    8.04911693e-16  -5.27355937e-16]]\n",
      "[[ -3.78903471e+00  -1.83068579e+00  -1.41742314e+00 ...,  -7.58144306e-02\n",
      "   -2.57183001e-02   1.78125904e-15]\n",
      " [ -4.26955623e-02  -4.53466655e-02   1.23680936e+00 ...,   5.01731336e-02\n",
      "    4.16938932e-02   1.78125904e-15]\n",
      " [  2.71617048e+00   2.17866221e+00   5.09924908e-02 ...,   3.61975143e-02\n",
      "    4.59511488e-02   1.78125904e-15]\n",
      " ..., \n",
      " [  6.66173509e-01  -1.61663419e+00  -1.41993564e+00 ...,  -4.44368833e-02\n",
      "    1.17391601e-01   1.78125904e-15]\n",
      " [  4.13616575e+00  -5.24364576e-01  -3.29453482e+00 ...,   1.21874186e-01\n",
      "    7.38704650e-02   1.78125904e-15]\n",
      " [ -5.14791206e+00  -2.36561060e+00  -4.64042169e-01 ...,   1.27055184e-01\n",
      "    9.51574680e-02   1.78125904e-15]]\n"
     ]
    }
   ],
   "source": [
    "class PCA:\n",
    "    def __init__(self, n_components=None):\n",
    "        self.eigens_ = None\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X):\n",
    "        X = np.array(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        if self.n_components:\n",
    "            n_components = self.n_components\n",
    "        else:\n",
    "            n_components = n_features\n",
    "        \n",
    "#         print np.matmul(X.T, X)\n",
    "#         print np.matmul(X.T, X) / n_samples \n",
    "        print \"----------------------------------------\"\n",
    "        covariance = np.matmul(X.T, X) / n_samples\n",
    "        print len(covariance), len(covariance[0])\n",
    "        U, S, V = np.linalg.svd(covariance)\n",
    "        self.eigens_ = V[:n_components]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = np.array(X)\n",
    "        return np.matmul(self.eigens_, X.T)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        X = np.array(X)\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "\n",
    "pca = PCA()\n",
    "print pca.fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA as PCA_\n",
    "pca_ = PCA_()\n",
    "print pca_.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from tensorflow.contrib.tensor_forest.client import random_forest\n",
    "from tensorflow.contrib.tensor_forest.python import tensor_forest\n",
    "from tensorflow.python.estimator.canned import head as head_lib\n",
    "from tensorflow.python.estimator.inputs import numpy_io\n",
    "from tensorflow.python.feature_column import feature_column_lib as core_feature_column\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops.losses import losses\n",
    "from tensorflow.python.platform import test\n",
    "from tensorflow.python.training import checkpoint_utils\n",
    "\n",
    "class RandomForestRegressor:\n",
    "    def __init__(self, num_trees, num_features, num_nodes=1000):\n",
    "        self.num_trees = num_trees\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        hparams = tensor_forest.ForestHParams(\n",
    "            num_trees=num_trees,\n",
    "            max_nodes=num_nodes,\n",
    "            num_classes=1,\n",
    "            num_features=num_features,\n",
    "            regression=True)\n",
    "        \n",
    "        self.regressor = random_forest.TensorForestEstimator(hparams.fill())\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X).astype(np.float32)\n",
    "        y = np.asarray(y).astype(np.float32)\n",
    "        \n",
    "        train_input_fn = numpy_io.numpy_input_fn(\n",
    "            x={'data': X}, y=y, num_epochs=None, shuffle=False)\n",
    "        \n",
    "        self.regressor.fit(input_fn=input_fn, steps=500)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X).astype(np.float32)\n",
    "        predict_input_fn = numpy_io.numpy_input_fn(\n",
    "            x={'data': X}, y=None, num_epochs=1, shuffle=False)\n",
    "        \n",
    "        return map(lambda x: x['scores'], list(self.regressor.predict(input_fn=predict_input_fn)))\n",
    "\n",
    "data = pd.read_csv('data-LOFapplied.csv')\n",
    "jokes = list(data.columns)[list(data.columns).index('joke1'):len(data.columns)]\n",
    "X = data.drop(jokes[75], axis=1)\n",
    "y = data[jokes[75]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)\n",
    "\n",
    "regressor = RandomForestRegressor(num_trees=50, num_features=185)\n",
    "regressor.fit(X_train, y_train)\n",
    "pred = regressor.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print pred\n",
    "print mean_squared_error(pred, y_test)\n",
    "print r2_score(pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
